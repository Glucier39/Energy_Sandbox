Geddy Lucier
PSCI4999 Final Script
Electricity Price Predictions:

```{r}
library(tidyverse)
library(stargazer)
library(xgboost)
library(lubridate)
library(readr)
library(riem)
library(stringr)
library(gmodels)
library(caret)
library(data.table)
library(nnet)
library(rpart)
library(NeuralNetTools)
library(neuralnet)
library(ggplot2)
library(reshape2)
library(plotly)
library(glmnet)
```
Data Read In 
```{r}
#4/20/24 is start date 
phl_da <- read_csv("https://raw.githubusercontent.com/Glucier39/Energy_Sandbox/refs/heads/main/4999/data/da_hrl_lmps.csv")

phl_loads <- read_csv("https://raw.githubusercontent.com/Glucier39/Energy_Sandbox/refs/heads/main/4999/data/hrl_load_metered.csv") 
 
#zone_agg <- rbind(zones24, zones25)

phl_intersect <- intersect(phl_da$datetime_beginning_ept, phl_loads$datetime_beginning_ept)

phl_da <- phl_da %>% filter(datetime_beginning_ept %in% phl_intersect) # %>% select(-datetime_beginning_utc, -datetime_beginning_ept)
phl_loads <- phl_loads %>% filter(datetime_beginning_ept %in% phl_intersect) %>% select(mw)

phl_agg <- cbind(phl_da, phl_loads)


phl_agg$date_raw <- substr(phl_agg$datetime_beginning_ept, 1, 9)
####################################################################

# Look up PHL
stations <- riem_stations("PA_ASOS")  # Returns stations in Pennsylvania
stations[grep("PHL", stations$id), ]

# Download data
phl_weather <- riem_measures(
  station = "PHL", 
  date_start = "2024-04-20", 
  date_end = "2025-04-20"
) 

phl_weather$date_raw <- substr(phl_weather$valid, 1, 10)
phl_weather$date_raw <- str_replace_all(phl_weather$date_raw, "-", "/") 
phl_weather_clean <- phl_weather %>% 
  distinct(date_raw, .keep_all = TRUE) 

phl_weather_clean <- phl_weather_clean %>%
  mutate(date_raw = format(as.Date(date_raw, format = "%Y/%m/%d"), "%m/%d/%Y")) %>%
  mutate(date_raw = sub("^0", "", date_raw),            # remove leading 0 from month
         date_raw = sub("/0", "/", date_raw))           # remove leading 0 from day

phl_agg <- phl_agg %>% dplyr::select(datetime_beginning_ept, pnode_name, total_lmp_da, congestion_price_da, marginal_loss_price_da, mw, date_raw) 



phl_weather_clean <- phl_weather_clean %>% 
  dplyr::select(-valid) %>% mutate(date_raw = as.character(date_raw))

phl_agg <- phl_agg %>% mutate(date_raw = as.Date(date_raw, format = "%m/%d/%Y"))# needs to be done last one merging is completed

phl_agg <- phl_agg %>%
  mutate(day_of_year = yday(date_raw))


phl_weather_clean <- phl_weather_clean %>% mutate(date_raw = as.Date(date_raw, format = "%m/%d/%Y"))# needs to be done last one merging is completed

phl_weather_clean <- phl_weather_clean %>%
  mutate(day_of_year = yday(date_raw))

phl_tot <- left_join(phl_agg, phl_weather_clean, by = "day_of_year") 

phl_tot <- phl_tot %>% dplyr::select(datetime_beginning_ept, day_of_year, total_lmp_da, congestion_price_da, marginal_loss_price_da, mw, tmpf, dwpf, relh)


```
Renwable Mix 
```{r}
solar <- read_csv("https://raw.githubusercontent.com/Glucier39/Energy_Sandbox/refs/heads/main/4999/data/solar_gen.csv") 
wind <- read_csv("https://raw.githubusercontent.com/Glucier39/Energy_Sandbox/7ad8d913af08a482a3c6bb01f90582b1c6904f01/4999/data/wind_gen.csv") 

solar <- solar %>% filter(area == "MIDATL") %>% dplyr::select(solar_generation_mw)


renewable <- cbind(solar, wind)

intr <- intersect(phl_tot$datetime_beginning_ept, renewable$datetime_beginning_ept)

renewable <- renewable %>% filter(datetime_beginning_ept %in% intr) %>% dplyr::select(wind_generation_mw, solar_generation_mw)

phl_tot <- cbind(phl_tot, renewable) # factor in renwable energy generation 

phl_tot <- data.table(phl_tot)

# Create shifted future price (1 day ahead) for total_lmp_da
phl_tot[, FutureLMP := shift(total_lmp_da, 24, type = "lead")] # 24 rows ahead - is it predictive 

# Calculate % gain from current to future price
phl_tot[, FutureLMP_PercentGain := ((FutureLMP - total_lmp_da) / total_lmp_da) * 100]

# Create binary outcome: 1 if future price is higher than current, else 0
phl_tot[, FutureLMP_Outcome := ifelse(FutureLMP > total_lmp_da, 1, 0)]

phl_tot$day_of_year <- as.numeric(phl_tot$day_of_year)

phl_tot <- phl_tot %>% select(-datetime_beginning_ept, -FutureLMP_PercentGain, -FutureLMP, -congestion_price_da, -marginal_loss_price_da,) # so only features are still present

#final cleaning steps 
phl_tot<-data.frame(phl_tot)

phl_tot <- phl_tot[complete.cases(phl_tot), ]


#write_csv(phl_tot, file = "Documents/GitHub/Energy_Sandbox/4999/phl_tot_electricity.csv")
```


XGBOOST model 
```{r}
set.seed(150)
index <- createDataPartition(phl_tot$FutureLMP_Outcome, p=0.8, list=FALSE)
        train_DecisionDT <- phl_tot[index, ]
        test_DecisionDT <- phl_tot[-index, ]
        
FeaturesToUse<-setdiff(names(train_DecisionDT),"FutureLMP_Outcome")

X <- as.matrix(train_DecisionDT[, FeaturesToUse])
y <- train_DecisionDT$FutureLMP_Outcome

Xtest <- as.matrix(test_DecisionDT[, FeaturesToUse])
ytest <- test_DecisionDT$FutureLMP_Outcome
  
  
dtrain <- xgb.DMatrix(data = X, label = y)

params <- list(
  objective = "binary:logistic",  # for binary classification
  eval_metric = "logloss",        # evaluation metric
  max_depth = 4,                  # tree depth (you can adjust this)
  eta = 0.1                       # learning rate
)

nrounds <- 1000  # number of boosting rounds

watchlist <- list(train = dtrain) 
set.seed(130)
bst <- xgb.train(params = params, data = dtrain, nrounds = nrounds,
                 watchlist = watchlist, verbose = 0)

# Get predicted probabilities
pred_probs <- predict(bst, newdata = Xtest)

# Convert probabilities to binary class predictions
pred_labels <- ifelse(pred_probs > 0.5, 1, 0)

xg_confusion <- confusionMatrix(
  factor(pred_labels, levels = c(0, 1)),
  factor(ytest, levels = c(0, 1))
)

xg_confusion
```


```{r}
# Step 1: Set up features (X) and outcome (y)
train_x <- as.matrix(train_DecisionDT[, c("total_lmp_da", "day_of_year", "mw", "tmpf", "dwpf", "relh", "wind_generation_mw", "solar_generation_mw")])
train_y <- train_DecisionDT$FutureLMP_Outcome

test_x <- as.matrix(test_DecisionDT[, c("total_lmp_da", "day_of_year", "mw", "tmpf", "dwpf", "relh", "wind_generation_mw", "solar_generation_mw")])
test_y <- test_DecisionDT$FutureLMP_Outcome

# Step 2: Cross-validate to find best lambda
set.seed(100)  # For reproducibility
cv_lasso <- cv.glmnet(train_x, train_y, alpha = 1, family = "binomial")

# Best lambda value
best_lambda <- cv_lasso$lambda.min

# Step 3: Train final Lasso model with best lambda
lasso_model <- glmnet(train_x, train_y, alpha = 1, family = "binomial", lambda = best_lambda)

# Step 4: Make predictions on test set
# Get predicted probabilities
pred_probs <- predict(lasso_model, newx = test_x, type = "response")

# Turn probabilities into 0/1 predictions (threshold at 0.5)
pred_classes <- ifelse(pred_probs > 0.5, 1, 0)

# Step 5: Evaluate
confusionMatrix(factor(pred_classes), factor(test_y))




```

```{r}
coef_lasso <- coef(lasso_model)
coef_df <- as.data.frame(as.matrix(coef_lasso))
coef_df$Feature <- rownames(coef_df)
colnames(coef_df) <- c("Coefficient", "Feature")

# Filter out zero coefficients and intercept
coef_df <- coef_df %>% filter(Coefficient != 0, Feature != "(Intercept)")

# Plot
ggplot(coef_df, aes(x = Coefficient, y = reorder(Feature, Coefficient))) +
  geom_point(size = 3) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Lasso Coefficient Plot", 
       x = "Coefficient Estimate", 
       y = "Feature") +
  theme_minimal(base_size = 14)

```
```{r}
importance_matrix <- xgb.importance(model = bst)
# Plot importance
xgb.plot.importance(importance_matrix, top_n = 10)
importance_df <- as.data.frame(importance_matrix)

# Plot using ggplot
ggplot(importance_df, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "XGBoost Feature Importance (by Gain)", 
       x = "Feature", 
       y = "Importance (Gain)") +
  theme_minimal(base_size = 14)
```






```{r}
eval_log <- bst$evaluation_log

p_eval <- plot_ly(data = eval_log,
                  x = ~iter,
                  y = ~train_logloss,
                  type = 'scatter',
                  mode = 'lines+markers')

p_eval <- layout(p_eval,
                 title = "Training Logloss over Boosting Iterations",
                 xaxis = list(title = "Iteration"),
                 yaxis = list(title = "Logloss"))

# Display the plot
p_eval
library(DiagrammeR)

tree_plot <- xgb.plot.tree(model = bst, trees = 0)

# Convert the DiagrammeR graph to an SVG string...
svg_str <- DiagrammeRsvg::export_svg(tree_plot)

# ...then convert the SVG to a PNG saved in a temporary file.
tmp_file <- tempfile(fileext = ".png")
rsvg::rsvg_png(charToRaw(svg_str), tmp_file)

# Convert the PNG file to a base64 URI for embedding into a plotly image trace.
img_uri <- base64enc::dataURI(file = tmp_file, mime = "image/png")

# Create a plotly figure that shows the tree image.
p_tree <- plot_ly() %>%
  layout(title = "XGBoost Model Tree",
         images = list(
           list(
             source = img_uri,
             x = 0, y = 1,
             sizex = 1, sizey = 1,
             xref = "x",
             yref = "y",
             sizing = "contain",
             opacity = 1,
             layer = "above"
           )
         ),
         xaxis = list(visible = FALSE),
         yaxis = list(visible = FALSE))
         
# Display the tree plot
p_tree

importance_matrix <- xgb.importance(model = bst)

# View top variables
print(importance_matrix)

# Plot importance nicely

```


```{r}


top_features <- importance_matrix[1:8, ]
top_features_long <- top_features %>%
  pivot_longer(cols = c(Gain, Cover, Frequency),
               names_to = "Metric",
               values_to = "Value")


ggplot(top_features_long, aes(x = reorder(Feature, Value), y = Value, fill = Metric)) +
  geom_col(position = "stack") +  # <-- STACK instead of dodge
  coord_flip() +
  labs(title = "XGBOOST Feature Importance Metrics",
       x = "Feature",
       y = "Combined Value",
       fill = "Metric") +
  theme_minimal(base_size = 14)
```






